name: anythingllm

networks:
  anything-llm:
    driver: bridge

volumes:
  ollama1_data: # Named volumes are better for persistence
  ollama2_data:

services:
  anything-llm:
    container_name: anythingllm_multiuser
    build:
      context: ../.
      dockerfile: ./docker/Dockerfile
      args:
        ARG_UID: ${UID:-1000}
        ARG_GID: ${GID:-1000}
    cap_add:
      - SYS_ADMIN
    user: "${UID:-1000}:${GID:-1000}"
    depends_on: # Wait for NGINX to be ready
      - nginx
    ports:
      - "3001:3001"
    env_file:
      - .env
    environment:
      - NODE_ENV=production
      - ANYTHING_LLM_RUNTIME=docker
      - STORAGE_DIR=/app/server/storage
      - JWT_SECRET=${JWT_SECRET}
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_URL=http://nginx:8080
      - SIG_KEY=${SIG_KEY}
      - SIG_SALT=${SIG_SALT}
      - MULTI_USER_MODE=true
      - COMMUNITY_HUB_BUNDLE_DOWNLOADS_ENABLED=true
    volumes:
      - "../server/storage:/app/server/storage"
      - "../collector/hotdir/:/app/collector/hotdir"
      - "../collector/outputs/:/app/collector/outputs"
    networks:
      - anything-llm
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped

  ollama1:
    image: ollama/ollama:latest
    container_name: ollama1
    gpus: all # Both instances will share available GPUs
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=4 # Key for concurrent processing
    expose: # Expose port 11434 to other containers on the network
      - "11434"
    volumes:
      - ollama1_data:/root/.ollama
    networks:
      - anything-llm
    restart: unless-stopped

  ollama2:
    image: ollama/ollama:latest
    container_name: ollama2
    gpus: all
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=4 # Key for concurrent processing
    expose:
      - "11434"
    volumes:
      - ollama2_data:/root/.ollama
    networks:
      - anything-llm
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    container_name: ollama-lb
    depends_on:
      - ollama1
      - ollama2
    ports:
      - "8080:8080" # Expose the load balancer on host port 8080 for debugging
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    networks:
      - anything-llm
    restart: unless-stopped
