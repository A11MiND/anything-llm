# nginx.conf
worker_processes  auto;

events {
    worker_connections  1024;
}

http {
    upstream ollama_backends {
        # Use the backend with the least active connections
        least_conn;
        # Define the two Ollama servers
        server ollama1:11434 max_fails=3 fail_timeout=5s;
        server ollama2:11434 max_fails=3 fail_timeout=5s;
    }

    server {
        listen 8080; # Listen on port 8080 inside the container

        location / {
            # Optimize for long-running LLM requests
            proxy_http_version 1.1;
            proxy_set_header Connection "";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            # Very important long timeouts for LLM generation
            proxy_read_timeout 600s;
            proxy_send_timeout 600s;

            # Route the request to the upstream group
            proxy_pass http://ollama_backends;
        }
    }
}
